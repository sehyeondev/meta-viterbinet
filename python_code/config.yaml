# general
run_name: 'meta_training_120_rand_ind'

# coding parameters
channel_blocks: 1
use_ecc: False # True, False
n_symbols: 6

# channel
memory_length: 4
channel_type: 'ISI_AWGN' #'ISI_AWGN','Poisson'
noisy_est_var: 0 # 0, 0.1 for ISI_AWGN, 0.08 for Poisson
fading_in_channel: True # True, False
fading_in_decoder: True # True, False

# gamma values
gamma_start: 0.2
gamma_end: 0.2
gamma_num: 1

# validation hyperparameters
val_block_length: 120
val_SNR_start: 12
val_SNR_end: 12
val_SNR_step: 2
val_words: 100
eval_mode: 'aggregated' # 'aggregated','by_word'

# train hyperparameters
train_block_length: 120
train_minibatch_num: 50
train_minibatch_size: 120
train_SNR_start: 12
train_SNR_end: 12
train_SNR_step: 2
lr: 0.001 # learning rate
loss_type: 'CrossEntropy' # 'BCE','CrossEntropy','MSE'
print_every_n_train_minibatches: 5
optimizer_type: 'Adam' # 'Adam','RMSprop','SGD'
early_stopping_mode: False # True, False
self_supervised: False # True, False

# seed
noise_seed: 3450002
word_seed: 7860002

# meta
meta_words: 200
meta_lr: 0.001
support_size: 120